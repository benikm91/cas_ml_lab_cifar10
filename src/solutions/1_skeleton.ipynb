{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skeleton\n",
    "\n",
    "This skeleton should give you a good starting point.\n",
    "We load the training data and write the submission file needed for Kaggle.\n",
    "So, you can fully concentrate on data analysis and creating models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.952664,
     "end_time": "2020-11-12T13:41:11.650914",
     "exception": false,
     "start_time": "2020-11-12T13:41:10.698250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009145,
     "end_time": "2020-11-12T13:41:11.669935",
     "exception": false,
     "start_time": "2020-11-12T13:41:11.660790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare data\n",
    "\n",
    "First, we will load and transform our data. The transformation could be anything from splitting the data, adding new features, or basic feature engineering. In this skeleton, we only split the data into the train set (X_train, y_train) and validation set (X_val, y_val)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.092779,
     "end_time": "2020-11-12T13:41:11.789446",
     "exception": false,
     "start_time": "2020-11-12T13:41:11.696667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data and split into features and labels\n",
    "with open('../../data/train.pkl', 'rb') as f:\n",
    "    data_train = pickle.load(f)\n",
    "X_data = data_train[\"images\"]\n",
    "y_data = data_train[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.044926,
     "end_time": "2020-11-12T13:41:11.874527",
     "exception": false,
     "start_time": "2020-11-12T13:41:11.829601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split features and labels into train (X_train, y_train) and validation set (X_val, y_val).\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "We recommend you always start with data analysis to understand the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Do your data analysis here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009172,
     "end_time": "2020-11-12T13:41:11.893381",
     "exception": false,
     "start_time": "2020-11-12T13:41:11.884209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define and train model\n",
    "\n",
    "After the data analysis, we are ready to train a model. We recommend starting with a very simple model. We can try more complex ones later.\n",
    "\n",
    "Try for example a Logistic Regression, Decision Tree, or Linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.009193,
     "end_time": "2020-11-12T13:41:11.931002",
     "exception": false,
     "start_time": "2020-11-12T13:41:11.921809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Define and train your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009119,
     "end_time": "2020-11-12T13:41:11.968176",
     "exception": false,
     "start_time": "2020-11-12T13:41:11.959057",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After we have trained our model, we will evaluate it on the validation set. Note that we are working with a multi-class classification setting. Suitable metrics for multi-class classification are for example accuracy, precision, recall, or F1 score. They can all be imported from sklearn.\n",
    "\n",
    "It is good practice to compare you first result to a random baseline. In a classification case, it makes sense not to predict random classes but to always predict the majority class (most frequent class) in the training data. Here, this amounts to an accuracy of roughly 10% since the classes are balanced (if you haven't verified that, go back to your data analysis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.016847,
     "end_time": "2020-11-12T13:41:11.994562",
     "exception": false,
     "start_time": "2020-11-12T13:41:11.977715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deer\n"
     ]
    }
   ],
   "source": [
    "counts = Counter(y_train.flatten())\n",
    "mode = counts.most_common()[0][0]\n",
    "print(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.09456"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_pred = np.repeat(mode, y_val.shape[0])\n",
    "accuracy_score(y_true=y_val, y_pred=y_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refit the model on the entire training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make use of all possible data, we retrain the model on the entire training set (including the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frog\n"
     ]
    }
   ],
   "source": [
    "counts = Counter(y_data.flatten())\n",
    "mode_train_val = counts.most_common()[0][0]\n",
    "print(mode_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009323,
     "end_time": "2020-11-12T13:41:12.032676",
     "exception": false,
     "start_time": "2020-11-12T13:41:12.023353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict classes for test set\n",
    "\n",
    "If we are happy with the performance of our model on the validation set, we can apply it to the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/test.pkl', 'rb') as f:\n",
    "    X_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = np.repeat(mode_train_val, X_test.shape[0])\n",
    "y_test_pred_df = pd.DataFrame(y_test_pred, columns=['label'])\n",
    "print(y_test_pred_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To submit the predictions to Kaggle we write them into a .csv file, which you can manually submit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "papermill": {
     "duration": 0.016861,
     "end_time": "2020-11-12T13:41:12.125868",
     "exception": false,
     "start_time": "2020-11-12T13:41:12.109007",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test_pred_df.to_csv('../../out/train_mode_submission.csv', header=True, index_label='id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "papermill": {
   "duration": 6.853286,
   "end_time": "2020-11-12T13:41:13.194189",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-12T13:41:06.340903",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}